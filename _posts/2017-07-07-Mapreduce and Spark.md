---
layout: page
title: Mapreduce and Spark
tags: [Distributed]
excerpt_separator: <!--more-->
typora-root-url: ../
---

## MapReduce: Simplified Data Processing on Large Clusters

### å¼•è¨€

 è¿™ä¸¤ç¯‡Paperå¥½åƒä¹‹å‰å°±çœ‹è¿‡ã€‚å¦å¤–Mapreduceå’ŒSparkäº§ç”Ÿçš„å½±å“è¿˜æ˜¯å¾ˆå¤§çš„ï¼Œåç»­çš„å‘å±•ä¹Ÿå¾ˆå¤šï¼Œè¿™é‡Œå°±åªçœ‹çœ‹å®ƒä»¬åŸå§‹çš„è®ºæ–‡é‡Œé¢æåˆ°çš„ä¸€äº›ä¸œè¥¿å§ã€‚Mapreduceåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç¡®å®æ˜¯ä¸€ä¸ªé‡Œç¨‹ç¢‘æ€§è´¨çš„ä½œå“sï¼Œæœ€åé¢ç›¸å…³çš„ç³»ç»Ÿç ”ç©¶äº§ç”Ÿäº†éå¸¸å¤§çš„å½±å“ã€‚

### ç¼–ç¨‹æ¨¡å‹

  Mapreduceä¸­ä»»åŠ¡éƒ½è¢«æŠ½è±¡ä½mapå’Œreduceä¸¤ä¸ªéƒ¨åˆ†ï¼Œ

```
map (k1,v1) â†’ list(k2,v2)
reduce (k2,list(v2)) â†’ list(v2)
```

ä¸€ä¸ªåŸºæœ¬çš„word countçš„ä¾‹å­ï¼Œ

```
  map(String key, String value):
    // key: document name
    // value: document contents
    for each word w in value:
      EmitIntermediate(w, "1");
  reduce(String key, Iterator values):
    // key: a word
    // values: a list of counts
    int result = 0;
    for each v in values:
      result += ParseInt(v);
    Emit(AsString(result));
```

 è¿™ä¸ªä¾‹å­åº”è¯¥æ˜¯éå¸¸å‡ºåäº†ã€‚

### å®ç°

  Mapreduceæ‰§è¡Œçš„æ–¹å¼å¤§è‡´ä¸Šè¡¨ç¤ºå¦‚ä¸‹å›¾ã€‚åœ¨Googleå®ç°Mapreduceçš„æ—¶å€™ï¼Œä¸€èˆ¬çš„æœåŠ¡å™¨è¿˜æ˜¯åŒæ ¸çš„CPUã€2åˆ°4GBçš„å†…å­˜100Mbåˆ°1Gbçš„ç½‘å¡ï¼Œå’Œä»Šå¤©çš„å·®åˆ«è¿˜æ˜¯å¾ˆå¤§çš„ã€‚ä¸€èˆ¬çš„ä¸€ä¸ªMapreduceç³»ç»Ÿæ˜¯æœ‰å‡ ç™¾orå‡ åƒå°çš„æœºå™¨ç»„æˆçš„ã€‚Mapreduceä¸­å‡ ä¸ªç‚¹ï¼Œ

* æ•°æ®äº¤ç»™è‹¥å¹²å°çš„æœºå™¨å¤„ç†ï¼Œé¦–å…ˆè¦åšçš„å°±æ˜¯å°†æ•°æ®åˆ†åŒºã€‚Mapreduceä¼šå°†è¾“å…¥åˆ†è£‚æˆ16Måˆ°64Mçš„å—ã€‚
* åœ¨æ¥ä¸‹æ¥çš„Mapæ­¥éª¤ï¼Œä¸€ä¸ªæœºå™¨ä¸Šè¿è¡Œçš„ç¨‹åºä¼šç§°ä¸ºMasterï¼Œå…¶å®ƒçš„æˆä¸ºWorkerã€‚Workerçš„å·¥ä½œæœ‰Masteråˆ†é…ã€‚
* Workeråœ¨æ¥å—åˆ°ä»»åŠ¡ä¹‹åï¼Œå»è¯»å–å¯¹åº”çš„æ•°æ®ï¼Œåœ¨æ‰§è¡Œç”¨æˆ·å®šä¹‰çš„Mapå‡½æ•°ï¼Œå¤„ç†æ•°æ®ã€‚å¤„ç†çš„æ•°æ®ä¼šå…ˆæš‚æ—¶æ”¾åœ¨å†…å­˜é‡Œï¼Œåœ¨å¾—åˆ°ä¸€å®šçš„é‡ä¹‹åå†™å…¥åˆ°ç£ç›˜ã€‚å†™å…¥çš„æ—¶å€™ä¼šæ ¹æ®Reduceçš„Workerçš„æ•°é‡å¯¹Mapé˜¶æ®µçš„è¾“å‡ºè¿›è¡Œåˆ†åŒºã€‚Workerä¹Ÿä¼šæŠŠè¿™äº›è¾“å‡ºçš„ä½ç½®ä¿¡æ¯æŠ¥å‘Šç»™Masterï¼›
* Masteré€šçŸ¥Reduceé˜¶æ®µçš„Workerä¸Šä¸€é˜¶æ®µçš„è¾“å…¥æ•°æ®çš„ä½ç½®ä¿¡æ¯ã€‚ç„¶åReduceçš„Workerä½¿ç”¨RPCä»Mapé˜¶æ®µçš„æœºå™¨ä¸Šé¢è¯»å–å…¶æœ¬åœ°ç£ç›˜ä¸Šé¢çš„è¾“å‡ºã€‚åœ¨è¯»å–äº†æ‰€æœ‰çš„æ•°æ®ä¹‹åï¼Œä¼šå¯¹ä¸­é—´æ•°æ®è¿›è¡Œæ’åºï¼›
* Reduceé˜¶æ®µçš„Workerè¿™äº›ä¸­é—´æ•°æ®ï¼ŒæŒ‡å‘ç”¨æˆ·å®šä¹‰çš„Reduceå‡½æ•°ã€‚è¾“å…¥è¿™éƒ¨åˆ†çš„ç»“æœã€‚
* åœ¨æ‰€æœ‰çš„Mapå’ŒReduceä»»åŠ¡éƒ½å®Œæˆä¹‹åã€‚æ‰§è¡Œå›åˆ°ç”¨æˆ·ç¨‹åºï¼›åœ¨ç»“æŸä¹‹åï¼Œè¾“å…¥çš„ç»“æœä¼šåˆ†å¸ƒåœ¨å¤šä¸ªæ–‡ä»¶ä¹‹ä¸­(Reduce Workerçš„æ•°é‡)ï¼›

  Masterä¸»è¦è´Ÿè´£ç›‘æ§Workersçš„çŠ¶æ€ã€Mapé˜¶æ®µè¾“å…¥çš„ä½ç½®ä»¥åŠéç©ºé—²çš„Workæœºå™¨çš„æ ‡è¯†ï¼Œé€šçŸ¥Reduce Workerå¤„ç†æ•°æ®ã€‚å¦å¤–å®¹é”™æ˜¯Mapreduceè®¾è®¡ä¸­é‡ç‚¹è€ƒè™‘çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚è¿™é‡Œåˆ†ä¸ºMasterå‡ºé”™å’ŒWorkerå‡ºé”™ã€‚å¯¹äºå‰è€…ï¼ŒMapreduceçš„å¤„ç†æ–¹å¼æ˜¯Masterä¼šå‘¨æœŸæ€§åœ°å°†Masterä¸­çš„æ•°æ®ç»“æ„å†™å…¥checkpointï¼Œå¦å¤–å¯åŠ¨ä¸€ä¸ªç„¶åä»checkpointæ¢å¤ã€‚Paperä¸­è®¤ä¸ºï¼ŒMasterå‡ºç°å¤±è´¥çš„å¯èƒ½æ€§å¿…é¡»ä½ï¼Œå½“æ—¶çš„å®ç°æ–¹å¼å°±æ˜¯ç®€å•ç»ˆæ­¢è¿è¡Œï¼Œå®¢æˆ·ç«¯å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦é‡è¯•æ“ä½œã€‚å¯¹äºWorkerçš„å¤±è´¥ï¼ŒMasteråœ¨å‘ç°Workerä¸€æ®µæ—¶é—´æ²¡æœ‰å“åº”ä¹‹åã€‚å®Œæˆçš„Mapä»»åŠ¡å¿…é¡»é‡æ–°æ‰§è¡Œï¼Œå¦å¤–Mapçš„æ‰§è¡Œçš„è¾“å‡ºåœ¨Mapæœºå™¨çš„æœ¬åœ°ç£ç›˜ã€‚è€ŒReduceé˜¶æ®µçš„å¤±è´¥åˆ™ä¸éœ€è¦ï¼Œå› ä¸ºå®ƒä»¬çš„è¾“å…¥ä¿å­˜åœ¨ä¸€ä¸ªå…¨å±€çš„æ–‡ä»¶ç³»ç»Ÿ(åº”è¯¥å°±æ˜¯GFS)ã€‚

![mapreduce-execution](/assets/img/mapreduce-execution.png)

### ä¼˜åŒ– & æ”¹è¿›

 Paperä¸­æåˆ°çš„åœ¨åŸºæœ¬çš„Mapreduceä¸Šçš„ä¸€äº›æ”¹è¿›orä¼˜åŒ–ï¼Œ

* æ”¯æŒè‡ªå®šä¹‰çš„åˆ†åŒºå‡½æ•°ï¼›

* å¯ä»¥å®šä¹‰Combinerå‡½æ•°ï¼Œç”¨äºéƒ¨åˆ†åˆå¹¶ä¸€äº›ä¸­é—´ç»“æœï¼›

* æ”¯æŒCounterï¼Œç”¨æ¥åšä¸€äº›ç»Ÿè®¡ä¿¡æ¯ï¼Œ

  ```
   Counter* uppercase;
    uppercase = GetCounter("uppercase");
    map(String name, String contents):
      for each word w in contents:
        if (IsCapitalized(w)):
          uppercase->Increment();
        EmitIntermediate(w, "1");
  ```

* æ”¯æŒè¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»å‹ï¼›

* Side-effectsï¼Œæ”¯æŒè¿è¡Œä¸­è¾“å‡ºä¸€äº›é¢å¤–çš„æ•°æ®ï¼›

* æ”¯æŒæœ¬åœ°æ‰§è¡Œæ–¹ä¾¿æµ‹è¯•ï¼›

* æ”¯æŒå¤„ç†çš„æ—¶å€™è·³è¿‡ä¸€äº›åçš„è®°å½•ã€‚

### æ€§èƒ½

  Paperä¸­æ€§èƒ½çš„æ•°æ®æ„Ÿè§‰åœ¨ä»Šå¤©å·²ç»æ²¡æœ‰å¤šå¤§çš„æ„ä¹‰äº†ã€‚

## Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing

### å¼•è¨€

  è¿™ç¯‡Paperæ˜¯Sparkæœ€åˆçš„è®ºæ–‡ï¼Œåªè¦è®¨è®ºçš„æ˜¯Sparkä¸­RDDæŠ½è±¡çš„ä¸€äº›ä¸œè¥¿ã€‚å½“ç„¶ç°åœ¨Sparkçš„å†…å®¹è¿œè¿œä¸æ­¢è¿™äº›ã€‚è¿™é‡Œåªçœ‹çœ‹è®ºæ–‡ä¸­å†™çš„ä¸€äº›ä¸œè¥¿,

```
... We show that Spark is up to 20Ã— faster than Hadoop for iterative applications, speeds up a real-world data analytics report by 40Ã—, and can be used interactively to scan a 1 TB dataset with 5â€“7s latency. More fundamentally, to illustrate the generality of RDDs, we have implemented the Pregel and HaLoop programming models on top of Spark, including the placement optimizations they employ, as relatively small libraries (200 lines of code each).
```

### RDDæŠ½è±¡

  RDDæ˜¯ä¸€ä¸ªåªè¯»çš„ã€åˆ†åŒºçš„æ•°æ®è®°å½•é›†åˆã€‚RDDåªèƒ½ä»ä»å­˜å‚¨çš„æ•°æ®ä¸­åˆ›å»ºå’Œå…¶å®ƒçš„RDDè½¬åŒ–è€Œæ¥ã€‚RDDä¸€ä¸ªå…³é”®çš„åœ°æ–¹å°±åœ¨ä¸RDDä¹‹é—´çš„è½¬æ¢(transformations)ï¼ŒSparkä¿æŠ¤è¯¸å¤šçš„è½¬æ¢çš„å‡½æ•°ã€‚RDDåœ¨ä»»ä½•æ—¶å€™éƒ½ä¸éœ€è¦å»å®ä¾‹åŒ–ï¼Œå®ƒçŸ¥é“å…¶çš„æ´¾ç”Ÿçš„å…³ç³»(lineage)ï¼Œåˆ©ç”¨è¿™ä¸ªlineageå¯ä»¥ä»å®ƒçš„ç‰©ç†å­˜å‚¨æ•°æ®çš„åˆ†åŒºè®¡ç®—å‡ºRDDã€‚å¦å¤–ï¼Œç”¨æˆ·è¿˜å¯ä»¥å¯¹RDDè¿›è¡ŒæŒä¹…åŒ–å’Œåˆ†åŒºæ“ä½œã€‚Sparkæ˜¯ä½¿ç”¨Scalaå†™å‡ºçš„ï¼Œè¾¨æçš„ä»£ç å¾ˆç²¾ç‚¼ï¼Œ

![spark-example](/assets/img/spark-example.png)

å¦å¤–ä¸€ä¸ªğŸŒ°ï¼Œ

```scala
// Count errors mentioning MySQL:
errors.filter(_.contains("MySQL")).count()
// Return the time fields of errors mentioning
// HDFS as an array (assuming time is field
// number 3 in a tab-separated format):
errors.filter(_.contains("HDFS"))
      .map(_.split(â€™\tâ€™)(3))
      .collect()
```

  RDDçš„è®¡ç®—æ—¶lazyçš„ï¼Œå³å»¶è¿Ÿè®¡ç®—ï¼Œåªæœ‰åœ¨çœŸçš„éœ€è¦è®¡ç®—å‡ºç»“æœä½¿ç”¨çš„æ—¶å€™æ‰ä¼šå®é™…è¿›è¡Œè®¡ç®—æ“ä½œã€‚æ“ä½œä¸€èˆ¬éƒ½åœ¨å†…å­˜ä¸­è¿›è¡Œï¼Œå¦‚æœå†…å­˜ä¸å¤Ÿï¼ŒRDDå¯ä»¥å†™åˆ°ç£ç›˜ä¸Šé¢ã€‚åœ¨RDDä¹‹é—´çš„è½¬æ¢ä¸Šé¢ï¼Œå­˜åœ¨å®½ä¾èµ–å’Œçª„ä¾èµ–çš„åŒºåˆ«ï¼Œå‰è€…åªçš„æ˜¯è½¬åŒ–è€Œæ¥çš„RDDä¾èµ–ä¸å‰ä¸€æ­¥çš„æ‰€æœ‰çš„RDDï¼Œè€Œåè€…æŒ‡çš„æ˜¯åªä¾èµ–ä¸å¸¸æ•°ä¸ªï¼Œä¸æ•°æ®è§„æ¨¡æ— å…³ï¼Œ

![spark-dependencies](/assets/img/spark-dependencies.png)

### ç¼–ç¨‹æ¥å£

  ä½¿ç”¨Sparkçš„æ–¹å¼æ—¶ç¼–å†™driverç¨‹åºï¼Œdriverä¼šå’Œå¤šä¸ªWorkerè¿æ¥ã€‚Driverå®šä¹‰è‹¥å¹²çš„RDDä»¥åŠåœ¨è¿™äº›RDDä¸Šé¢çš„æ“ä½œï¼ŒSparkçš„ä»£ç åŒ…å«äº†è¿™äº›RDDçš„lineageã€‚Workersæ—¶é•¿æ—¶é—´è¿è¡Œçš„è¿›ç¨‹ï¼Œä¿å­˜å’Œå¤„ç†RDDã€‚

![spark-runtime](/assets/img/spark-runtime.png)

  ä¸å‰é¢çš„Mapreduceå°±æ˜¯Mapå’ŒReduceä¸¤ä¸ªæ“ä½œï¼ŒSparkä¸­çš„æ“ä½œä¸°å¯Œåœ°å¤šï¼Œ

![spark-transformations](/assets/img/spark-transformations.png)

å‡ ä¸ªğŸŒ°ï¼š**é€»è¾‘å›å½’**ï¼Œ

```scala
val points = spark.textFile(...)
                  .map(parsePoint).persist()
var w = // random initial vector
for (i <- 1 to ITERATIONS) {
  val gradient = points.map{ p =>
    p.x * (1/(1+exp(-p.y*(w dot p.x)))-1)*p.y
  }.reduce((a,b) => a+b)
  w -= gradient
}
```

**PageRank**

```scala
val links = spark.textFile(...).map(...).persist()
var ranks = // RDD of (URL, rank) pairs
for (i <- 1 to ITERATIONS) {
  // Build an RDD of (targetURL, float) pairs
  // with the contributions sent by each page
  val contribs = links.join(ranks).flatMap {
    (url, (links, rank)) =>
      links.map(dest => (dest, rank/links.size))
  }
  // Sum contributions by URL and get new ranks
  ranks = contribs.reduceByKey((x,y) => x+y)
                .mapValues(sum => a/N + (1-a)*sum)
 }
```

### å®ç°çš„ä¸€äº›ä¿¡æ¯

  åœ¨è®ºæ–‡ä¸­Sparkç‰ˆæœ¬çš„å®ç°åªç”¨äº†14000å¤šè¡Œçš„Scalaä»£ç ã€‚ç³»ç»Ÿè¿è¡Œåœ¨Mesosæœºå™¨ç®¡ç†å™¨ä¹‹ä¸Šã€‚Paperä¸­å…³äºSparkå®ç°çš„ä¸€äº›ä¿¡æ¯è®¨è®ºäº†4ç‚¹ï¼Œ

* ä»»åŠ¡è°ƒåº¦ï¼ŒSparkçš„ä»»åŠ¡è°ƒåº¦ä¹Ÿå……åˆ†åˆ©ç”¨äº†RDDçš„linageã€‚è°ƒåº¦æ ¹æ®RDDçš„linageæ„å»ºä¸€ä¸ªä¿æŠ¤å¤šä¸ªstageçš„DAGã€‚è°ƒåº¦è¿™é‡Œä¼šå°è¯•è®©ä¸€ä¸ªStageé‡Œé¢åŒ…å«æ›´åŠ å¤šçš„çª„ä¾èµ–å…³ç³»ï¼Œä½¿å®ƒä»¬æµæ°´çº¿åŒ–ã€‚è°ƒåº¦çš„æ—¶å€™ä¹Ÿä¼šè€ƒè™‘åˆ°å±€éƒ¨æ€§ï¼Œå‡å°‘é€šä¿¡å¼€é”€ã€‚å¦å¤–ï¼Œå¯¹äºå®½ä¾èµ–ï¼Œçˆ¶åˆ†åŒºçš„èŠ‚ç‚¹ä¼šå°†ä¸­é—´ç»“æœå®ä¾‹åŒ–(ç‰©åŒ–)ï¼Œç®€åŒ–å®¹é”™å¤„ç†ï¼Œæœ‰ç‚¹ç±»å‹ä¸Mapreduceä¸­Mapé˜¶æ®µçš„å¤„ç†æ–¹å¼ï¼Œ

![spark-sheduling](/assets/img/spark-sheduling.png)

* äº¤äº’å¼ä¸€ä¸ªè§£é‡Šç¨‹åºï¼Œè¿™é‡Œå°±æ˜¯æŒ‡å®‰è£…Sparkä¹‹åå¯ä»¥åœ¨å‘½ä»¤è¡Œä¸‹é¢ä½¿ç”¨çš„ä¸€ä¸ªäº¤äº’å¼çš„å·¥å…·ï¼›
* å†…å­˜ç®¡ç†ï¼ŒSparkæ”¯æŒä¸‰ç§çš„RDDå­˜å‚¨æ–¹å¼ï¼šä»¥ä¸€èˆ¬çš„Javaå¯¹è±¡ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œç³»åˆ—åŒ–ä¿å­˜åœ¨å†…å­˜ä¸­ä»¥åŠä¿å­˜åœ¨ç£ç›˜ä¸­ã€‚ç¬¬ä¸€ç§æ€§èƒ½æœ€å¥½ï¼Œä½†æ˜¯ç”±äºJVMçš„ä¸€äº›åŸå› è¿™ä¸ªæ¯”è¾ƒæ¶ˆè€—å†…å­˜ã€‚ç”±äºå†…å­˜æœ‰é™ï¼ŒSparkä¹Ÿå¯ä»¥å°†RDDä¿å­˜ç£ç›˜ä¸Šé¢å»ï¼Œåœ¨è¿™é‡Œä¹Ÿä½¿ç”¨äº†å¸¸ç”¨çš„LRUçš„æ·˜æ±°æœºåˆ¶ï¼›
* Checkpointæ”¯æŒï¼ŒSparkçš„Linageæœºåˆ¶å¯ä»¥å®¹æ˜“å­åœ¨æ•…éšœä¹‹åæ¢å¤æ•°æ®ï¼Œä½†æ˜¯æœ‰äº›æ—¶å€™å¯èƒ½å¾ˆæ¶ˆè€—æ—¶é—´ã€‚è¿™é‡ŒSparkæ”¯æŒæ£€æŸ¥ç‚¹çš„æ–¹æ³•ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„APIã€‚

### è¯„ä¼°

  è¿™é‡Œçš„å…·ä½“ä¿¡æ¯å¯ä»¥å‚çœ‹[1],

![spark-perf](/assets/img/spark-perf.png)

## å‚è€ƒ

1. MapReduce: Simplified Data Processing on Large Clusters, OSDI â€™04.
2. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing, NSDI'12.